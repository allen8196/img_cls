# src/main.py
import os
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import wandb

# åŒ¯å…¥æœ¬åœ°æ¨¡çµ„
from src.dataset import create_patient_level_split, ChestXRayDataset, get_transforms
from src.model import create_lora_efficientnetv2
from src.loss import FocalLoss
from src.engine import train_one_epoch, evaluate
from src.utils import save_checkpoint, load_checkpoint
from src.config import config

def main():
    # ä½¿ç”¨ config ä¸­çš„è£ç½®è¨­å®š
    device = config.DEVICE
    print(f"ä½¿ç”¨è£ç½®: {device}")
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)

    # åˆå§‹åŒ– W&B Run ---
    # å°‡ config ç‰©ä»¶è½‰æ›ç‚ºå­—å…¸ä»¥ä¾¿ wandb è¨˜éŒ„ (æ’é™¤å…§å»ºå±¬æ€§)
    config_dict = {
        k: getattr(config, k) 
        for k in dir(config) 
        if not k.startswith('__') and not callable(getattr(config, k))
    }
    
    try:
        wandb.init(
            project="Pneumonia-Classification-EfficientNetV2", # å°ˆæ¡ˆåç¨±
            config=config_dict, # è¨˜éŒ„æ‰€æœ‰è¶…åƒæ•¸
            name=f"run_img{config.IMAGE_SIZE}_bs{config.BATCH_SIZE}_lr{config.LEARNING_RATE}_lora{config.LORA_RANK}" # è‡ªè¨‚ Run åç¨±
        )
        print("âœ… Weights & Biases ç›£æ§å·²å•Ÿå‹•ã€‚")
    except Exception as e:
        print(f"âŒ ç„¡æ³•åˆå§‹åŒ– W&B (è«‹æª¢æŸ¥ API é‡‘é‘°æ˜¯å¦è¨­å®š): {e}")
        wandb.init(mode="disabled") # å³ä½¿å¤±æ•—ä¹Ÿç¹¼çºŒåŸ·è¡Œ (ç¦ç”¨ W&B)


    # --- [SWEEP] ç²å– Sweeps åƒæ•¸ä¸¦è¦†è“‹ config ---
    # wandb.config æœƒåŒ…å« default config å’Œ sweep å‚³å…¥çš„æ–°åƒæ•¸
    # æˆ‘å€‘ç”¨ wandb.config çš„å€¼ä¾† *æ›´æ–°* æˆ‘å€‘çš„ config ç‰©ä»¶
    print("--- [SWEEP] æ­£åœ¨æª¢æŸ¥ä¸¦æ‡‰ç”¨ Sweep åƒæ•¸ ---")
    try:
        # [é‡è¦] å¾ wandb.config å›å¯«åˆ° config ç‰©ä»¶
        config.LEARNING_RATE = wandb.config.LEARNING_RATE
        config.LORA_RANK = wandb.config.LORA_RANK
        config.LORA_ALPHA = wandb.config.LORA_ALPHA
        config.FOCAL_ALPHA = wandb.config.FOCAL_ALPHA
        
        # æ›´æ–° Run åç¨±ä»¥åæ˜  Sweep åƒæ•¸
        sweep_name = f"sweep_lr{config.LEARNING_RATE:.0e}_rank{config.LORA_RANK}_alpha{config.LORA_ALPHA:.2f}"
        wandb.run.name = sweep_name
        
        print(f"  [SWEEP] æˆåŠŸæ‡‰ç”¨ Sweep åƒæ•¸ã€‚")
        print(f"  [SWEEP] å­¸ç¿’ç‡: {config.LEARNING_RATE}")
        print(f"  [SWEEP] LoRA Rank: {config.LORA_RANK}")
        print(f"  [SWEEP] LoRA Alpha: {config.LORA_ALPHA}")
        print(f"  [SWEEP] Focal Alpha: {config.FOCAL_ALPHA}")
    except AttributeError as e:
        print(f"  [SWEEP] æœªåŸ·è¡Œ Sweep (æˆ–åƒæ•¸åç¨±ä¸ç¬¦)ï¼Œä½¿ç”¨ config.py é è¨­å€¼ã€‚")
    # ------------------------------------------------


    # 1. è³‡æ–™æº–å‚™
    print("æ­£åœ¨æº–å‚™è³‡æ–™é›†...")
    # æ¥æ”¶å®Œæ•´çš„ train, val, test åˆ‡åˆ†
    (train_files, train_labels), (val_files, val_labels), (test_files, test_labels) = \
        create_patient_level_split(
            config.DATA_DIR, 
            test_size=config.TEST_SPLIT_SIZE,
            val_size_ratio=config.VAL_SPLIT_SIZE
        )
    
    # ç²å–å½±åƒè½‰æ›
    train_transform, val_transform = get_transforms(image_size=config.IMAGE_SIZE)
    
    # å»ºç«‹ Dataset
    train_dataset = ChestXRayDataset(train_files, train_labels, transform=train_transform)
    val_dataset = ChestXRayDataset(val_files, val_labels, transform=val_transform)
    
    # å»ºç«‹ Test Dataset
    test_dataset = ChestXRayDataset(test_files, test_labels, transform=val_transform)
    
    # å»ºç«‹ DataLoader
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=True, 
        num_workers=config.NUM_WORKERS, 
        pin_memory=config.PIN_MEMORY
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=False, 
        num_workers=config.NUM_WORKERS, 
        pin_memory=config.PIN_MEMORY
    )
    
    # å»ºç«‹ Test DataLoader
    test_loader = DataLoader(
        test_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=False, 
        num_workers=config.NUM_WORKERS, 
        pin_memory=config.PIN_MEMORY
    )
    
    print(f"è¨“ç·´é›†: {len(train_dataset)} æ¨£æœ¬, {len(train_loader)} æ‰¹æ¬¡")
    print(f"é©—è­‰é›†: {len(val_dataset)} æ¨£æœ¬, {len(val_loader)} æ‰¹æ¬¡")
    print(f"æ¸¬è©¦é›†: {len(test_dataset)} æ¨£æœ¬, {len(test_loader)} æ‰¹æ¬¡")

    # 2. æ¨¡å‹ã€æå¤±å‡½æ•¸ã€å„ªåŒ–å™¨
    print("æ­£åœ¨å»ºç«‹æ¨¡å‹...")
    # ç§»é™¤å¤šé¤˜çš„å›å‚³å€¼
    model = create_lora_efficientnetv2(
        num_classes=config.NUM_CLASSES, 
        lora_rank=config.LORA_RANK, 
        lora_alpha=config.LORA_ALPHA
    )
    model.to(device)
    
    # æå¤±å‡½æ•¸
    criterion = FocalLoss(alpha=config.FOCAL_ALPHA, gamma=config.FOCAL_GAMMA)
    
    # å„ªåŒ–å™¨ (åƒ…å„ªåŒ–å¯è¨“ç·´çš„åƒæ•¸)
    optimizer = AdamW(
        model.parameters(), 
        lr=config.LEARNING_RATE, 
        weight_decay=config.WEIGHT_DECAY
    )
    
    # å­¸ç¿’ç‡èª¿åº¦å™¨ (ç›£æ§é©—è­‰é›†çš„ F1-score)
    scheduler = ReduceLROnPlateau(
        optimizer, 
        mode='max', # 'max' å› ç‚ºæˆ‘å€‘ç›£æ§ F1-score
        factor=0.1, 
        patience=config.SCHEDULER_PATIENCE,
    )
    
    # ç›£æ§æ¨¡å‹æ¢¯åº¦èˆ‡åƒæ•¸ ---
    wandb.watch(model, criterion, log="all", log_freq=100) # æ¯ 100 æ‰¹æ¬¡è¨˜éŒ„ä¸€æ¬¡

    # 3. è¨“ç·´è¿´åœˆ
    best_f1: float = 0.0
    best_model_path = os.path.join(config.OUTPUT_DIR, 'best_model.pth')
    # åˆå§‹åŒ– early_stopping è¨ˆæ•¸å™¨ 
    early_stopping_counter: int = 0
    print(f"--- é–‹å§‹è¨“ç·´ï¼Œå…± {config.EPOCHS} å€‹ Epoch ---")

    for epoch in range(config.EPOCHS):
        print(f"\n--- Epoch {epoch+1}/{config.EPOCHS} ---")
        
        # è¨“ç·´
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
        print(f"Epoch {epoch+1} è¨“ç·´æå¤±: {train_loss:.4f}")
        
        # é©—è­‰
        metrics = evaluate(model, val_loader, criterion, device)
        print(f"Epoch {epoch+1} é©—è­‰æŒ‡æ¨™:")
        for k, v in metrics.items():
            print(f"  {k}: {v:.4f}")

        # è¨˜éŒ„æŒ‡æ¨™
        # æº–å‚™è¦ log çš„è³‡æ–™å­—å…¸
        log_data = {
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "val_loss": metrics['loss'],
            "val_accuracy": metrics['accuracy'],
            "val_f1_score": metrics['f1_score'],
            "val_auc": metrics['auc'],
            "val_precision": metrics['precision'],
            "val_recall": metrics['recall'],
            "learning_rate": optimizer.param_groups[0]['lr'] # è¨˜éŒ„ç•¶å‰å­¸ç¿’ç‡
        }
        wandb.log(log_data)
            
        # å„²å­˜èˆ‡æª¢æŸ¥é‚è¼¯
        current_f1 = metrics['f1_score']
        
        if current_f1 > best_f1:
            best_f1 = current_f1
            print(f"ğŸš€ æ–°é«˜ F1-score: {best_f1:.4f}ã€‚å„²å­˜æ¨¡å‹è‡³ {best_model_path}...")
            save_checkpoint(model, best_model_path)
            # å„²å­˜æœ€ä½³ F1-score åˆ° summary
            wandb.run.summary["best_val_f1_score"] = best_f1
            
            # é‡ç½® early_stopping è¨ˆæ•¸å™¨
            early_stopping_counter = 0 
        else:
            # æœªè¦‹æ”¹å–„ï¼Œè¨ˆæ•¸å™¨+1
            early_stopping_counter += 1
            print(f"Epoch {epoch+1} æœªè¦‹æ”¹å–„. Early Stopping è¨ˆæ•¸: {early_stopping_counter}/{config.EARLY_STOPPING_PATIENCE}")

        # æ›´æ–°å­¸ç¿’ç‡ (åœ¨ F1 æª¢æŸ¥ä¹‹å¾Œ)
        scheduler.step(current_f1)
        
        # æª¢æŸ¥æ˜¯å¦è§¸ç™¼ early_stopping
        if early_stopping_counter >= config.EARLY_STOPPING_PATIENCE:
            print(f"--- è§¸ç™¼ Early Stopping (Patience={config.EARLY_STOPPING_PATIENCE}) ---")
            # [WANDB] è¨˜éŒ„åœæ­¢çš„ Epoch
            wandb.run.summary["stopped_epoch"] = epoch + 1
            wandb.log({"early_stopped": True})
            break # è·³å‡º epoch è¿´åœˆ

    # 4. æœ€çµ‚æ¸¬è©¦
    print("\n--- è¨“ç·´å®Œæˆ ---")
    print(f"è¼‰å…¥æœ€ä½³æ¨¡å‹ (F1: {best_f1:.4f}) é€²è¡Œæœ€çµ‚æ¸¬è©¦...")
    
    # å»ºç«‹ä¸€å€‹æ–°çš„æ¨¡å‹å¯¦ä¾‹ä¸¦è¼‰å…¥æœ€ä½³æ¬Šé‡
    # (ç¢ºä¿æˆ‘å€‘ä½¿ç”¨çš„æ˜¯æœ€ä½³æ¨¡å‹ï¼Œè€Œä¸åƒ…æ˜¯æœ€å¾Œä¸€å€‹ Epoch çš„æ¨¡å‹)
    final_model = create_lora_efficientnetv2(
        num_classes=config.NUM_CLASSES, 
        lora_rank=config.LORA_RANK, 
        lora_alpha=config.LORA_ALPHA
    )
    load_checkpoint(final_model, best_model_path, device)
    
    # åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°
    test_metrics = evaluate(final_model, test_loader, criterion, device)
    
    print("\n--- æœ€çµ‚æ¸¬è©¦æŒ‡æ¨™ (Test Set) ---")
    for k, v in test_metrics.items():
        print(f"  {k}: {v:.4f}")
    
    # è¨˜éŒ„æœ€çµ‚æ¸¬è©¦çµæœ
    test_log_data = {f"test_{k}": v for k, v in test_metrics.items()}
    wandb.log(test_log_data)
    # å°‡æ¸¬è©¦æŒ‡æ¨™å„²å­˜åˆ° Run çš„ Summary ä¸­
    for k, v in test_metrics.items():
        wandb.run.summary[f"final_test_{k}"] = v

    wandb.finish()

    print("\nå°ˆæ¡ˆåŸ·è¡Œå®Œç•¢ã€‚")

if __name__ == '__main__':
    main()